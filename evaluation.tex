\section{Evaluation}
\label{sec.evaluation}
To demonstrate that our proposed solution for container forensic auditing with UnPAK and SmashPAK is practical and efficient, 
we conducted a series of tests to answer the following questions: 
\begin{itemize}
	\item Can we build the popular paths kernel profile for containers in a systematic way? (Section~{\ref{sec.evaluation.1}})
	\item Can an analysis of the popular paths data for containers help developers and researchers perform efficient security auditing? (Section~{\ref{sec.evaluation.2}})
	\item Can real-world containers run on the UnPAK auditing kernel with no loss of functionality? (Section~{\ref{sec.evaluation.3}})
	\item What is the performance overhead of using UnPAK to audit, as opposed to using the original Linux kernel? (Section~{\ref{sec.evaluation.4}})
	\item Is forensic auditing using SmashPAK efficient? (Section~{\ref{sec.evaluation.5}})
\end{itemize}

\subsection{Can popular paths kernel profiles for containers be built in a systematic way?}
\label{sec.evaluation.1} 
Our UnPAK auditing kernel relies on the popular paths data to locate the places where auditing is likely to reveal a previous bug exploit. 
Having a systematic way to build the popular paths kernel profile for containers is key to making its use scalable, reproducible, and accessible to developers and the research community. 
Therefore, the first step in our investigation was developing and testing such a standard procedure for containers. 
This procedure can be divided into three stages: selecting the \textbf{dataset}, assembling \textbf{a profiling framework}, and conducting the \textbf{experiment}. 

\textbf{The dataset:} The dataset stage involved selecting the containers to use and the workload that would run inside of  them. 
We wanted the defined dataset to be representative of the containers used by millions of clients, so we sourced a set accepted as widely used according to trustworthy real-world statistics.  
We selected 50 containers deemed most popular on Docker Hub according to their official number of pulls. Each selected container had more than 10 million pulls \cite{DockerHub}. 
The workload run by each was the set of commands or operations defined in the official Dockerfile ``CMD'' section for each container image.

\textbf{The profiling framework:} This next stage involved establishing a running environment and capturing the actual data, a procedure that required a robust framework. 
Ideally we wanted the framework to be portable so it could be used across different platforms. 
We selected the LinuxKit VM, which works with Linux, Windows, and Mac OS, and a Linux kernel with a Gcov profiling tool enabled to run the Docker containers. 
This framework works with  other Linux kernel versions, which makes the procedure more widely applicable. 

\textbf{The experiment:}  The test itself consisted of running the containers with their defined workloads in the profiling framework, and extracting the resulting  data. 
The procedure was able to automate the experimental runs in a way that allowed users to define key variables of the input, including the exact version of the container image to use, 
and the number of iterations to run for each container. By making  these design choices flexible, 
it was better able to accommodate container application users and developers with different needs and computing resources. In our experiment, 
we decided to do 10 iterations, because it provides a sufficient number of runs to make the data reliable, without consuming too much time and computing resources. 
In our experiment, we were able to automatically collect the popular paths data based on a training dataset of 50 popular Docker containers to build our kernel profile, 
which was then used to produce our UnPAK auditing kernel. 

\subsection{Can the popular paths data for containers help developers and researchers perform efficient security auditing?}
\label{sec.evaluation.2} 
Using the procedures described above, we obtained data for some of the most frequently accessed  containers on Docker Hub, and used it to check for security vulnerabilities. 
We started with a list of all 50 of the CVE kernel vulnerabilities for Linux kernel version 4.14.x that were available from the National Vulnerability Database when our study 
was conducted (July 2019) \cite{NVD} and identified the kernel patch that fixed the bug. 
This allowed us to identify the source line numbers, functions and files that corresponded to it, and use this information to highlight where these bugs were located in the kernel.   

It should be noted that 49 out of the 50 bugs tested in this experiment were discovered and confirmed \textit{after} publication of the original popular paths study \cite{Lock-in-Pop}. 
This shows that the metric can indeed effectively predict where bugs are likely to occur, and thus provide a solid basis for guiding efficient kernel auditing.  

The raw data generated by Gcov in the profiling framework was at the line-of-code level, as this is the level at which  the initial popular path study \cite{Lock-in-Pop} was performed. 
We wanted, however, to generate  additional information at the function and file level. 
Having these three different levels of granularity provides a full map of how the kernel code was used by containers and where the vulnerabilities might lie in each.

In the discussion below, we label files and functions with four different categories based on the popularity data. 
Those labeled ``unpop'': contain 0 lines of popular code; ``all-pop'': contain 100\% popular code lines; ``partial-pop'': contain at least 1 line of popular code 
and 1 line of unpopular code. ``Pop'' is the combined lines of all-pop + partial-pop.

\noindent
\textbf{Popular paths data at the file level}
\newline
For developers and security teams, the first thing to identify is which files are popular, as this can be a first step to secure the kernel without needing to 
deal with the complex logic and branches inside of each file. 

From our data (shown in Table \ref{tab:cve_files})), we can see that the ``all-pop'' files contain no bugs  and thus are the safest ones to use.  
Alternatively, the  ``unpop'' files contain more than half of the bugs. The higher bug density of the unpopular files suggests that removing these files could be an easy first step to securing the kernel. 

 ``Partial-pop'' files, which accounted for about 30\% of the bugs investigated, present a trickier problem to resolve. 
 The presence of some popular lines means these items are being used and can not simply be removed. If stricter security requirements are desired, 
 we need to go inside of these files to look at a finer granularity, which is the function level. 
 
\begin{table}
\small
\caption{CVE bugs located in Unpopular and Popular kernel files}
\label{tab:cve_files}
\begin{tabular}{l|r|r|r|r|r}
 kernel & unpop & pop & all-pop & partial-pop & total \\
 dir & \color{red}{(CVEs)} & \color{red}{(CVEs)} & \color{red}{(CVEs)} & \color{red}{(CVEs)} & \\
 \hline
 arch & 133\color{red}{(3)} & 272\color{red}{(1)} & 81 & 191\color{red}{(1)} & 405 \\
 \hline
 block & 10 & 44 & 0 & 44 & 54 \\
 \hline
 crypto & 53\color{red}{(1)} & 89\color{red}{(2)} & 0 & 89\color{red}{(2)} & 142 \\
 \hline
 drivers & 161\color{red}{(11)} & 543\color{red}{(3)} & 5 & 538\color{red}{(3)} & 704 \\
 \hline
 fs & 52\color{red}{(6)} & 176\color{red}{(2)} & 5 & 171\color{red}{(2)} & 228 \\
 \hline
 ipc & 1 & 9 & 0 & 9 & 10 \\
 \hline
 kernel & 40\color{red}{(2)} & 183 & 2 & 181 & 223 \\
 \hline
 mm & 16\color{red}{(1)} & 59\color{red}{(4)} & 0 & 59\color{red}{(4)} & 75 \\
 \hline
 net & 266\color{red}{(5)} & 482\color{red}{(3)} & 0 & 482\color{red}{(3)} & 748 \\
 \hline
 security & 6 & 17 & 0 & 17 & 23 \\
 \hline
 total & 738\color{red}{(29)} & 1874\color{red}{(15)} & 93 & 1781\color{red}{(15)} & 2612 \\
 \hline 
 percent & 28.3\% & 71.7\% & 3.6\% & 68.1\% & 100\% \\
 & \color{red}{(66\%)} & \color{red}{(34\%)} & \color{red}{(0\%)} & \color{red}{(34\%)} &
\end{tabular}
\textit{\textbf{unpop}: contain 0 line of popular code.} \\
\textit{\textbf{all-pop}: every line is popular code.} \\
\textit{\textbf{partial-pop}: contain at least 1 line of popular code and 1 line of unpopular code.} \\
\textit{\textbf{pop}: all-pop + partial-pop.}
\end{table}

\noindent
\textbf{Popular paths data at the function level}
\newline

Using the raw data, shown in Table \ref{tab:cve_functions}, we pinpoint which functions were used by popular containers. 
Our data shows that ``unpop'' functions contain 5x more bugs than ``pop'' functions, while the ``all-pop'' functions contain only one bug. 

As with the file analysis above, having this information can help security teams eliminate some risks by just avoiding the less-used functions. 
Unfortunately, that still leaves the ``partial-pop'' functions, which  contain 12 bugs.  As with the ``partial-pop'' files, 
the presence of some frequently used functions in this group means they cannot  simply be removed at this level. 
While analyzing each function at the line-of-code level is the most labor intensive of the methods elaborated here, it offers the best opportunity to remove risky code. 
Below we look at vulnerabilities revealed by the popular paths data at this most fine-grained level. 

\begin{table}
\small
\caption{CVE bugs located in Unpopular and Popular kernel functions}
\label{tab:cve_functions}
\begin{tabular}{l|r|r|r|r|r}
 kernel & unpop & pop & all-pop & partial-pop & total \\
 dir & \color{red}{(CVEs)} & \color{red}{(CVEs)} & \color{red}{(CVEs)} & \color{red}{(CVEs)} & \\
 \hline
 arch & 1528\color{red}{(7)} & 968\color{red}{(1)} & 312 & 656\color{red}{(1)} & 2496 \\
 \hline
 block & 777 & 264 & 68 & 196 & 1041 \\
 \hline
 crypto & 527\color{red}{(6)} & 121 & 36 & 85 & 648 \\
 \hline
 drivers & 6258\color{red}{(20)} & 2640 & 562 & 2078 & 8898 \\
 \hline
 fs & 1888\color{red}{(13)} & 1653\color{red}{(3)} & 559 & 1094\color{red}{(3)} & 3541 \\
 \hline
 ipc & 138 & 102 & 33 & 69 & 240 \\
 \hline
 kernel & 3562\color{red}{(8)} & 2280 & 793 & 1487 & 5842 \\
 \hline
 mm & 976\color{red}{(2)} & 889\color{red}{(6)} & 281 & 608\color{red}{(6)} & 1865 \\
 \hline
 net & 3703\color{red}{(9)} & 2405\color{red}{(3)} & 695\color{red}{(1)} & 1710\color{red}{(2)} & 6108 \\
 \hline
 security & 179 & 201 & 104 & 97 & 380 \\
 \hline
 total & 19536\color{red}{(65)} & 11523\color{red}{(13)} & 3443\color{red}{(1)} & 8080\color{red}{(12)} & 31059 \\
 \hline 
 percent & 62.9\% & 37.1\% & 11.1\% & 26\% & 100\% \\
 & \color{red}{(83.3\%)} & \color{red}{(16.7\%)} & \color{red}{(1.3\%)} & \color{red}{(15.4\%)} &
\end{tabular}
\end{table}

\begin{table*}
  \begin{center}
    \caption{Evaluation of the CVE vulnerabilities at the line level}
    \label{tab:evaluation_cve}
    \begin{tabular}{c|l|c|c|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \textbf{\#} & \textbf{CVE ID} & \textbf{CVSS Score} & \textbf{Description} & \textbf{Detected in the}\\
       & & & & \textbf{LinuxKit Popular Paths}\\
      \hline
      1 & CVE-2019-10124 & 7.8 & denial of service, in mm/memory-failure.c & \ding{55}\\
      2 & CVE-2019-9213 & 4.9 & kernel NULL pointer dereferences, in mm/mmap.c & \ding{55}\\
      3 & CVE-2019-9003 & 7.8 & use-after-free & \ding{55}\\
      4 & CVE-2019-8956 & 7.2 & use-after-free & \ding{55}\\
      5 & CVE-2019-8912 & 7.2 & use-after-free & \ding{55}\\
      6 & CVE-2019-7308 & 7.5 & out-of-bounds speculation on pointer arithmetic & \ding{55}\\
      7 & CVE-2019-3701 & 7.1& privilege escalation & \ding{55}\\
      8 & CVE-2018-1000204 & 6.3 & copy kernel heap pages to the userspace & \ding{55}\\
      9 & CVE-2018-1000200 & 4.9 & NULL pointer dereference & \ding{55}\\
      10 & CVE-2018-1000026 & 6.8 & denial of service & \ding{55}\\
      11 & CVE-2018-20511 & 2.1 & privilege escalation & \ding{55}\\
      12 & CVE-2018-20169 & 7.2 & mishandle size checks & \ding{55}\\
      13 & CVE-2018-18690 & 4.9 & unchecked error condition & \ding{55}\\
      14 & CVE-2018-18445 & 7.2 & out-of-bounds memory access & \ding{55}\\
      15 & CVE-2018-18281 & 4.6 & improperly flush TLB before releasing pages & \ding{55}\\
      16 & CVE-2018-18021 & 3.6 & denial of service & \ding{55}\\
      17 & CVE-2018-16862 & 2.1 & the cleancache subsystem incorrectly clears an inode & \ding{55}\\
      18 & CVE-2018-16658 & 3.6 & local attackers could read kernel memory & \ding{55}\\
      19 & CVE-2018-16276 & 7.2 & privilege escalation & \ding{55}\\
      \color{red}{20} & \color{red}{CVE-2018-15594} & \color{red}{2.1} & \color{red}{spectre-v2 attacks against paravirtual guests} & \color{red}{\ding{51}}\\
      \color{red}{21} & \color{red}{CVE-2018-15572} & \color{red}{2.1} & \color{red}{userspace-userspace spectreRSB attacks} & \color{red}{\ding{51}}\\
      22 & CVE-2018-14646 & 4.9 & NULL pointer dereference & \ding{55}\\
      23 & CVE-2018-14634 & 7.2 & integer overflow, privilege escalation & \ding{55}\\
      24 & CVE-2018-14633 & 8.3 & stack buffer overflow & \ding{55}\\
      25 & CVE-2018-14619 & 7.2 & privilege escalation & \ding{55}\\
      26 & CVE-2018-13406 & 7.2 & integer overflow & \ding{55}\\
      27 & CVE-2018-12904 & 4.4 & privilege escalation & \ding{55}\\
      28 & CVE-2018-11508 & 2.1 & local user could access kernel memory & \ding{55}\\
      29 & CVE-2018-11412 & 4.3 & ext4 incorrectly allows external inodes for inline data & \ding{55}\\
      30 & CVE-2018-10940 & 4.9 & incorrect bounds check allows kernel memory access & \ding{55}\\
      31 & CVE-2018-10881 & 4.9 & denial of service & \ding{55}\\
      32 & CVE-2018-10880 & 7.1 & denial of service & \ding{55}\\
      33 & CVE-2018-10879 & 6.1 & use-after-free & \ding{55}\\
      34 & CVE-2018-10878 & 6.1 & denial of service & \ding{55}\\
      35 & CVE-2018-10074 & 4.9 & denial of service & \ding{55}\\
      36 & CVE-2018-10021 & 4.9 & denial of service & \ding{55}\\
      37 & CVE-2018-8781 & 7.2 & code execution in kernel space & \ding{55}\\
      38 & CVE-2018-6555 & 7.2 & denial of service & \ding{55}\\
      39 & CVE-2018-6554 & 4.9 & denial of service & \ding{55}\\
      40 & CVE-2018-5390 & 7.8 & denial of service & \ding{55}\\
      41 & CVE-2018-1130 & 4.9 & NULL pointer dereference & \ding{55}\\
      \color{red}{42} & \color{red}{CVE-2018-1120} & \color{red}{3.5} & \color{red}{denial of service} & \color{red}{\ding{51}}\\
      43 & CVE-2018-1118 & 2.1 & kernel memory leakage & \ding{55}\\
      44 & CVE-2018-1068 & 7.2 & write to kernel memory & \ding{55}\\
      45 & CVE-2017-1000410 & 5.0 & leaking data in kernel address space & \ding{55}\\
      46 & CVE-2017-1000407 & 6.1 & denial of service & \ding{55}\\
      47 & CVE-2017-1000405 & 6.9 & overwrite read-only huge pages & \ding{55}\\
      48 & CVE-2017-18224 & 1.9 & race condition, denial of service & \ding{55}\\
      49 & CVE-2017-18216 & 2.1 & NULL pointer dereference, denial of service & \ding{55}\\
      50 & CVE-2015-5327 & 4.0 & out-of-bounds memory read & \ding{55}\\
    \end{tabular}
  \end{center}
\end{table*}

\noindent
\textbf{Popular paths data at the line level}
\newline

An analysis of the gathered data at the line level shows that the popular lines make up about  20\% of the kernel codebase, 
and that only 6\% (or three) of the CVE bugs were detected in those lines (Table \ref{tab:evaluation_cve}.) 
These three bugs were in commonly used kernel code that, to the best of our knowledge, 
cannot be avoided by any existing security systems. We describe these three bugs in more detail below.  

It is important, to note, however, that two of these bugs  (CVE-2018-15594 and CVE-2018-15572) are Spectre-related hardware vulnerabilities, 
based on fundamental flaws in CPU’s data cache and speculative execution. 
These flaws affect many modern processors \cite{ProjectZero}, and software tools and compartmentalization techniques are not designed to prevent them. 
The only software-related bug is  CVE-2018-1120, which is related to the \texttt{mmap()} system call, and regularly accessed by virtualization systems. 

\noindent
\textbf{[CVE-2018-15594]} 
\newline
This bug lies in \texttt{arch/x86/kernel/paravirt.c} in the Linux kernel before 4.18.1. The source code mishandles certain indirect calls, 
making it easier for attackers to conduct Spectre-v2 attacks against paravirtual guests. 
In our tests, this bug was found in the  kernel trace data, and was reached frequently by programs running in LinuxKit. 
This occurs because the code inside of the \texttt{paravirt\_patch\_call()} function in \texttt{arch/x86/kernel/paravirt.c} is used to rewrite an indirect call with a direct call, 
an essential function used to support Linux kernel paravirtualization. 

\noindent
\textbf{[CVE-2018-15572]} 
\newline
The \texttt{spectre\_v2\_select\_mitigation}  \\ 
function in \texttt{arch/x86/kernel/cpu/bugs.c} in the Linux kernel before 4.18.1 does not always fill RSB upon a context switch. 
This makes it easier for attackers to conduct userspace-userspace spectreRSB attacks. 
As this piece of code in \\
\texttt{spectre\_v2\_select\_mitigation(void)} function would be used by every program as the kernel attempts to mitigate potential Spectre attacks, 
it explains why it appeared in our LinuxKit popular paths. 

\noindent
\textbf{[CVE-2018-1120]} 
\newline
This vulnerability was found affecting the Linux kernel before version 4.17. by 
\texttt{mmap()ing} a FUSE-backed file onto the memory of a process containing command line arguments (or environment strings). 
It appears in the popular paths because \texttt{mmap()} is a commonly used system call. 
Furthermore, this vulnerability involves \texttt{proc\_pid\_cmdline\_read()} and \texttt{environ\_read()} functions that are commonly invoked by virtual machines and user programs. 
According to CVSS 3.0 \cite{CVSS} metrics, this bug has ``high'' attack complexity, suggesting that it is very hard to exploit it in practice. 
Therefore, the negative impact of this bug’s existence is low. 

The fact that 94\% of bugs were located in the unpopular paths demonstrates that monitoring the unpopular paths to target kernel auditing is a correct choice, 
as an UnPAK kernel based on the unpopular paths would be able to catch the overwhelming majority of bugs. In addition, the 6\% of bugs missed were in the popular paths, 
and are used by almost every program during their normal runs, which means that they were quite easy to catch and thus far less interesting as auditing targets. 

An important result of this multi-level data analysis was the discovery  that users actually had a hierarchy of security choices, rather than one ``all or nothing'' option. 
Based on the sensitivity of the application and the resources available, users can actually choose the appropriate tradeoff of effort vs.security that is most appropriate for  security auditing. 

Line level is at a finer granularity, and is more precise about the location of bugs when providing kernel logs for auditing. 
Functions and file levels data could provide higher-level auditing logs that are much smaller in size, and thus can yield results with even less search time. 
Moreover, ``line'' level tends to be more ``volatile.'' Thus, this data could change frequently in newly released kernel versions, mandating updates that could be time consuming.  
``Function'' and ``file'' level data tend to be more stable, and could be valid for a longer time period. 

\noindent
\textbf{Are there sufficient commonalities across different containers to form a valid kernel profile for the most frequently used containers?} 
\newline
To ensure our kernel profile could be applicable to a broad variety of containers, we needed to establish that the data of containers we obtained shared common ground in their kernel footprints. 
If there were enough commonalities to form a realistic popular paths profile to support multiple popular containers, we could use the kernel trace, or the record of all kernel code executed, 
of just a sample of containers to represent the trace of many more. Such confirmation is essential for building a robust kernel profile to guide our container kernel auditing strategy.

To answer this question, we used the CDF (Cumulative Distribution Function) to analyze how soon the kernel trace of different containers would converge. 
Results of the CDF, as visualized in Figure \ref{fig:pp-cdf}, point out that the trace of the top six popular containers covered about 98\% of the total kernel trace for 25 containers. 
This offers additional corroboration that the popular paths data we collected is valid for hundreds of containers on Docker Hub.  

\begin{figure*}
\centering
\includegraphics[width=1.5\columnwidth]{diagram/pp-cdf.png}
\caption{\small CDF of the popular paths of Docker containers showing most share the same kernel footprint}
\label{fig:pp-cdf}
\end{figure*}

In addition to the always used popular paths, which we identified as the common kernel lines that appeared each and every time, 
we also discovered certain lines of code that showed up sporadically. For these lines, we looked at what activities they performed, and how common they were across different containers. 
In our analysis of the kernel trace of 10 frequently used Docker containers, we identified eight pieces of infrequently executed kernel code common among them.  
These traces are presented graphically in Figure \ref{fig:cdf-marked}. 
\begin{enumerate}
	\item \verb|kernel/cgroup/freezer.c|: a cgroup is freezing if any FREEZING flags are set.
	\item \verb|kernel/locking/rwsem-xadd.c|: waiting for a write lock to be granted. 
	\item \verb|kernel/locking/rwsem-xadd.c|: waiting for the read lock to be granted.
	\item \verb|mm/filemap.c|: process waitqueue for pages and check for a page match to prevent potential waitqueue hash collision. 
	\item \verb|fs/exec.c|: all other threads have exited, wait for the thread group leader to become inactive and to assume its PID. 
	\item \verb|kernel/workqueue|: insert a barrier work in the queue. According to the comments from the kernel source code, 
	the reason for inserting a barrier work seems to be to prevent a cancellation. 
	This is because \\
	\verb|try_to_grab_pending()| can not determine whether the work to be grabbed is at the head of the queue and thus can not clear LINKED flag of the previous work. 
	There must be a valid next work after a work with a LINKED flag set. 
	\item \verb|arch/x86/kernel/tsc.c|: try to calibrate the TSC \\ 
	against the Programmable Interrupt Timer and return the frequency of the TSC in kHz. 
	\item \verb|kernel/locking/mutex.c|: hand off a mutex. Give up ownership to a specific task, when @task = NULL, this is equivalent to a regular unlock.
\end{enumerate}

\begin{figure*}
\centering
\includegraphics[width=1.5\columnwidth]{diagram/cdf-marked.png}
\caption{\small Common kernel code identified across different containers}
\label{fig:cdf-marked}
\end{figure*}

What this analysis tells us is that, despite the fact that they are infrequently executed, this  code performs essential kernel functions and is used by multiple containers. 
Therefore, we considered them popular paths, and included them in our popular paths kernel profile. On closer examination, we learn they are not always executed during each run  because, 
as largely race-condition related codes (except for code \#7), they depend on locks, and on system conditions that may vary during different runs. 
Being able to capture these examples of infrequently-executed code and include them into our popular paths profile is important, because it makes the UnPAK unpopular kernel logs more accurate, 
and therefore is key to aiding efficient security auditing. 


\subsection{Can real-world containers run on the UnPAK auditing kernel with no loss of functionality?}
\label{sec.evaluation.3} 
To demonstrate that the UnPAK kernel can support efficient auditing in practice, we needed to confirm that it will not break any essential kernel functionalities, and can support real-world containers.

The next set of tests were to determine if  running applications in UnPAK, will cause any loss of functionality, as compared to using the original Linux kernel. 
The first study used the Linux Testing Project (LTP) [cite] test suites—an open source test project designed to validate the reliability, robustness, and stability of Linux—to verify that 
UnPAK functions as anticipated. The second test involved running a collection of the most popular containers from Docker Hub to verify if they were able to function correctly if they ran on UnPAK auditing kernel.

\subsubsection{Testing functionality with the LTP test suites}
\label{sec.evaluation.3.1} 
\textbf{Experimental Setup:} We used the Dockerfile provided by LinuxKit \cite{LinuxKit} to create the container image that runs the LTP version 20170116. 
This test project offers a set of regression and conformance tests designed to let members of the open source community confirm behavior of the Linux kernel. 
The test script we ran consisted of 798 test cases that verified the correctness of system functionalities, such as memory allocation, network connection, file system access, locking, and more. 
Using the test suites, we ran our experiments inside of a LinuxKit version 0.2+ virtual machine. The machine was built using Docker version 18.03.0-ce running on 
a host operating system of Ubuntu 16.04 LTS, with Linux kernel 4.13.0-36-generic. A QEMU emulator version 2.5.0 served as the local hypervisor. 

\textbf{Results:}  UnPAK was able to run all of the 798 test cases, produced the same results as the original unmodified Linux kernel. 
Among all these tests, 673 of them generated ``success'' as their expected output on both kernels. We observed 105 test cases returned ``failures'' as expected, 
mainly because of restrictions imposed by LinuxKit. For example, ``mem01'' test failed because the malloc function failed to allocate 3056 MB memory due to the default memory restriction; 
the ``gf01'' test failed due to ``no space left on device'' in LinuxKit. The ``swapon01'' test failed due to swapfile not available, and more. 
There were 20 tests skipped due to certain required functions being unavailable in the LinuxKit VM version we tested. 
For example, the swap file was not accessible in LinuxKit, therefore the related ``swapoff'' test iterations were skipped intentionally.

In summary, UnPAK yields the exact same output as the unmodified Linux kernel when running the 798 test cases in the LTP test suites. 
This shows that our UnPAK audit kernel implementation, using KVM hypercalls, can provide correct functionality.

\subsubsection{Testing functionality on real-world container applications}
\label{sec.evaluation.3.2} 
\textbf{Experimental Setup:} To confirm that UnPAK could run applications in real-world practice, we identified the 100 most downloaded containers from Docker Hub, 
and ran them in the same version of the LinuxKit virtual machine used in the test described in 4.3.1. Each container was run in the LinuxKit VM using the commands from its official Docker image. 
To take into account any potential variances, each container was run 10 times in the exact same environment. We used the top 50 popular containers as the training dataset to 
get the popular paths kernel profile, and then tested our UnPAK kernel with these 50 containers plus 50 more popular containers that were not used in building the kernel profile. 

\textbf{Results:}  We verified that the 100 containers run using UnPAK were able to finish their normal workloads with, on average, less than 1\% of runtime overhead. 
In doing so,  less than 0.1\% of the total kernel lines reached were unpopular. And these unpopular lines appeared both in the 50 containers used in the training dataset, 
as well as in the 50 new containers in the testing dataset. Using our list of 50 CVE bugs, we verified that these unpopular paths logged by UnPAK audit kernel contained none of the bugs. 
(These unpopular lines were mostly rarely executed race-condition related code) 

The results show that real-world containers will not generate unpopular kernel logs in the overwhelmingly majority of (>99.9\%) cases. 
This is expected, since no malicious party or attackers were actually trying to exploit any vulnerabilities in our testing. 
It shows that our UnPAK kernel works as expected when auditing on popular real-world containers. 

\subsection{Performance Evaluation}
\label{sec.evaluation.4} 
\textbf{Running real-world containers with UnPAK}
\newline
Adoption of UnPAK is dependent on its real-world viability. So, the next round of tests were to ensure it would not negatively affect performance overhead costs. 
To demonstrate this, we evaluated both the run-time performance and memory space overhead of the UnPAK audit kernel.

\textbf{Experimental Setup:}
We compared data from containers running on the original Linux kernel and on the 
UnPAK kernel and measured the overhead costs. We used the exact same running environment and configuration to ensure a fair comparison. 
In each test, the container finished its workload as defined in the official Dockerfile. We then measured the runtimes for both kernels and compared them. 

\textbf{Results:} 
On average, the results show running containers on UnPAK incurred about 0.5\% to 1\% of extra performance overhead, as compared to the original kernel. 
Results of the top 10 containers are shown in Figure \ref{fig:performance}. (For each container, the runtime was the average runtime of 10 runs.)

\begin{figure*}
\centering
\includegraphics[width=1.5\columnwidth]{diagram/performance.png}
\caption{\small Runtime performance comparison for the top 10 containers}
\label{fig:performance}
\end{figure*}

The original Linux kernel image used for the LinuxKit test  is sized at 163,012,008 bytes. In comparison, UnPAK is sized at 163,622,440 bytes. 
Thus the extra memory space added by our modified kernel was only about 0.37\%. 

In summary, for both runtime and memory space UnPAK has negligible overhead.

\subsection{Is forensic auditing using SmashPAK efficient?}
\label{sec.evaluation.5} 
Lastly, but most importantly, we needed to assess if our SmashPAK data structures could actually support  forensic auditing without any substantial drawbacks. 
To do so, we performed the  operations defined in Section 3, Step 3 on a collection of SmashPAK kernel data. 
The UnPAK kernel emits SmashPAKs at a specific frequency for each container and each one will have a small set of potential time periods.  
As the SmashPAKs from relevant containers are collected and combined into the overall collection, the total scope and span will grow. 

For each released kernel, there is a fixed amount of basic blocks for the kernel codebase. So each SmashPAK has a fixed length of indices. 
For the kernel we implemented, there is a total number of 158,929 unpopular basic blocks, so we have 158,929 indices for our SmashPAK. 
Each index points to a list of occurrences. The occurrences data structure has a maximum fixed size, so the total size of each SmashPAK is fixed and relatively small. 
For example, we set the maximum size of our occurrences list to 10, and use ``int'' (four bytes) to store the information. 
The total size of each SmashPAK is only around 10 mb. Considering that many SmashPAKs could be combined, 
modern machines or cloud services conducting security audits could easily store millions. 

We implemented SmashPAK with our kernel size (158929 unpopular basic blocks). 
The average time cost of a search operation for a CVE bug in SmashPAK was 0.1598 seconds. 
The average time cost of a combine operation for two SmashPAKs was 0.2422 seconds for almost empty (<=10\% filled) occurrences lists, 
and 0.8399 seconds for almost full (>=90\% filled) occurrences lists. 

Considering that the end user who conducts the audit is likely a human being, such as system administrator, 
committing only about 0.16 seconds searching to learn if (and when) a CVE  could have been triggered at a particular location for a container is very efficient in practice.